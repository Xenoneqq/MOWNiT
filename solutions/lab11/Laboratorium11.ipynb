{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e12d471",
   "metadata": {},
   "source": [
    "## Zadanie 2: Optymalizacja Ścieżki Robota z Omijaniem Przeszkód\n",
    "\n",
    "### 1. Wprowadzenie\n",
    "\n",
    "Zadanie 2 dotyczy wyznaczenia optymalnej ścieżki robota pomiędzy dwoma zadanymi punktami: początkowym $\\mathbf{x}(0)$ i końcowym $\\mathbf{x}(n)$. Kluczowym elementem problemu jest konieczność unikania przeszkód rozmieszczonych na trasie robota. Problem ten, będący nieliniowym zadaniem optymalizacji z ograniczeniami (unikanie kolizji), został przekształcony do problemu optymalizacji nieograniczonej poprzez odpowiednie zdefiniowanie funkcji kosztu. Ścieżka robota jest reprezentowana przez serię $n+1$ punktów $\\mathbf{x}(0), \\mathbf{x}(1), \\dots, \\mathbf{x}(n)$, gdzie każdy punkt $\\mathbf{x}(i) \\in \\mathbb{R}^2$. Punkty początkowy i końcowy są ustalone.\n",
    "\n",
    "Punkty reprezentujące przeszkody, $\\mathbf{r}(j) \\in \\mathbb{R}^2$, są zdefiniowane w macierzy przeszkód $\\mathbf{R} \\in \\mathbb{R}^{k \\times 2}$.\n",
    "\n",
    "### 2. Funkcja Celu\n",
    "\n",
    "Funkcja celu $F(\\mathbf{X})$, gdzie $\\mathbf{X} = [\\mathbf{x}(0), \\mathbf{x}(1), \\dots, \\mathbf{x}(n)]^T$, jest sformułowana jako suma dwóch głównych składników, z których każdy odpowiada za inne aspekty optymalizacji:\n",
    "\n",
    "$$F(\\mathbf{X}) = \\lambda_1 \\sum_{i=0}^{n} \\sum_{j=1}^{k} \\frac{1}{\\epsilon + \\| \\mathbf{x}(i) - \\mathbf{r}(j) \\|_2^2} + \\lambda_2 \\sum_{i=0}^{n-1} \\| \\mathbf{x}(i+1) - \\mathbf{x}(i) \\|_2^2$$\n",
    "\n",
    "gdzie:\n",
    "* **Pierwszy człon (z $\\lambda_1$)** odpowiada za **unikanie przeszkód**. Im bliżej punkt ścieżki $\\mathbf{x}(i)$ znajduje się przeszkody $\\mathbf{r}(j)$, tym większa jest wartość tego członu, \"karząc\" bliskość przeszkody. Stała $\\epsilon$ dodana do mianownika zapobiega dzieleniu przez zero, zapewniając stabilność numeryczną.\n",
    "* **Drugi człon (z $\\lambda_2$)** odpowiada za **minimalizację długości ścieżki**. Jest to suma kwadratów odległości między kolejnymi punktami na ścieżce, co skłania algorytm do znajdowania krótszych trajektorii.\n",
    "* **Stałe $\\lambda_1$ i $\\lambda_2$** to parametry wagowe, które określają względny wpływ każdego członu na ogólną wartość funkcji celu.\n",
    "* **$n$** to liczba odcinków ścieżki, a **$n+1$** to liczba punktów.\n",
    "* **$k$** to liczba przeszkód.\n",
    "\n",
    "### 3. Wyprowadzenie Wyrażenia na Gradient $\\nabla F$\n",
    "\n",
    "Do minimalizacji funkcji celu $F(\\mathbf{X})$ za pomocą metody największego spadku niezbędne jest analityczne wyznaczenie gradientu $\\nabla F$. Gradient jest wektorem pochodnych cząstkowych funkcji $F$ względem każdej współrzędnej każdego punktu $\\mathbf{x}(i)$ na ścieżce: $\\nabla F = \\left[ \\frac{\\partial F}{\\partial \\mathbf{x}(0)}, \\dots, \\frac{\\partial F}{\\partial \\mathbf{x}(n)} \\right]$.\n",
    "Pochodna cząstkowa względem pojedynczego punktu $\\mathbf{x}(p)$, gdzie $p \\in \\{0, \\dots, n\\}$, jest sumą pochodnych cząstkowych obu członów funkcji $F$.\n",
    "\n",
    "#### 3.1. Pochodna Członu Odpowiadającego za Unikanie Przeszkód\n",
    "\n",
    "Dla członu $F_1 = \\lambda_1 \\sum_{i=0}^{n} \\sum_{j=1}^{k} \\frac{1}{\\epsilon + \\| \\mathbf{x}(i) - \\mathbf{r}(j) \\|_2^2}$, pochodna względem $\\mathbf{x}(p)$ wynosi:\n",
    "\n",
    "$$\\frac{\\partial F_1}{\\partial \\mathbf{x}(p)} = -2 \\lambda_1 \\sum_{j=1}^{k} \\frac{\\mathbf{x}(p) - \\mathbf{r}(j)}{(\\epsilon + \\| \\mathbf{x}(p) - \\mathbf{r}(j) \\|_2^2)^2}$$\n",
    "\n",
    "#### 3.2. Pochodna Członu Odpowiadającego za Długość Ścieżki\n",
    "\n",
    "Dla członu $F_2 = \\lambda_2 \\sum_{i=0}^{n-1} \\| \\mathbf{x}(i+1) - \\mathbf{x}(i) \\|_2^2$, pochodna względem $\\mathbf{x}(p)$ zależy od położenia punktu $p$:\n",
    "\n",
    "* Dla punktu początkowego ($p=0$):\n",
    "    $$\\frac{\\partial F_2}{\\partial \\mathbf{x}(0)} = -2 \\lambda_2 (\\mathbf{x}(1) - \\mathbf{x}(0))$$\n",
    "* Dla punktu końcowego ($p=n$):\n",
    "    $$\\frac{\\partial F_2}{\\partial \\mathbf{x}(n)} = 2 \\lambda_2 (\\mathbf{x}(n) - \\mathbf{x}(n-1))$$\n",
    "* Dla punktów wewnętrznych ($0 < p < n$):\n",
    "    $$\\frac{\\partial F_2}{\\partial \\mathbf{x}(p)} = 2 \\lambda_2 (2\\mathbf{x}(p) - \\mathbf{x}(p-1) - \\mathbf{x}(p+1))$$\n",
    "\n",
    "#### 3.3. Pełne Wyrażenie na Gradient\n",
    "\n",
    "Sumując powyższe wyrażenia, gradient $\\frac{\\partial F}{\\partial \\mathbf{x}(p)}$ dla każdego punktu $\\mathbf{x}(p)$ wynosi:\n",
    "\n",
    "* Dla $p=0$:\n",
    "    $$\\frac{\\partial F}{\\partial \\mathbf{x}(0)} = -2 \\lambda_1 \\sum_{j=1}^{k} \\frac{\\mathbf{x}(0) - \\mathbf{r}(j)}{(\\epsilon + \\| \\mathbf{x}(0) - \\mathbf{r}(j) \\|_2^2)^2} - 2 \\lambda_2 (\\mathbf{x}(1) - \\mathbf{x}(0))$$\n",
    "* Dla $p=n$:\n",
    "    $$\\frac{\\partial F}{\\partial \\mathbf{x}(n)} = -2 \\lambda_1 \\sum_{j=1}^{k} \\frac{\\mathbf{x}(n) - \\mathbf{r}(j)}{(\\epsilon + \\| \\mathbf{x}(n) - \\mathbf{r}(j) \\|_2^2)^2} + 2 \\lambda_2 (\\mathbf{x}(n) - \\mathbf{x}(n-1))$$\n",
    "* Dla $0 < p < n$:\n",
    "    $$\\frac{\\partial F}{\\partial \\mathbf{x}(p)} = -2 \\lambda_1 \\sum_{j=1}^{k} \\frac{\\mathbf{x}(p) - \\mathbf{r}(j)}{(\\epsilon + \\| \\mathbf{x}(p) - \\mathbf{r}(j) \\|_2^2)^2} + 2 \\lambda_2 (2\\mathbf{x}(p) - \\mathbf{x}(p-1) - \\mathbf{x}(p+1))$$\n",
    "\n",
    "W implementacji, ze względu na stałe położenie punktów początkowego $\\mathbf{x}(0)$ i końcowego $\\mathbf{x}(n)$, ich gradienty są zerowane, aby algorytm optymalizacyjny nie zmieniał ich wartości.\n",
    "\n",
    "### 4. Algorytm Największego Spadku z Przeszukiwaniem Liniowym (Metoda Złotego Podziału)\n",
    "\n",
    "Minimalizacja funkcji celu $F(\\mathbf{X})$ została przeprowadzona za pomocą iteracyjnej **metody największego spadku (Gradient Descent)**. W każdej iteracji algorytm aktualizuje punkty ścieżki w kierunku ujemnego gradientu funkcji celu. Kluczowym elementem zapewniającym efektywną zbieżność jest zastosowanie **przeszukiwania liniowego (Line Search)**, które pozwala na wyznaczenie optymalnej długości kroku w kierunku gradientu. W tym zadaniu wykorzystano **metodę złotego podziału (Golden Section Search)** do przeszukiwania liniowego.\n",
    "\n",
    "#### 4.1. Opis Matematyczny Algorytmu\n",
    "\n",
    "1.  **Inicjalizacja:**\n",
    "    * Ustalana jest początkowa ścieżka $\\mathbf{X}^{(0)}$, z losowymi punktami wewnętrznymi $\\mathbf{x}(1), \\dots, \\mathbf{x}(n-1)$ oraz zadanymi $\\mathbf{x}(0)$ i $\\mathbf{x}(n)$.\n",
    "    * Określana jest maksymalna liczba iteracji (`max_iterations`).\n",
    "2.  **Iteracyjny Proces Minimalizacji:**\n",
    "    Dla każdej iteracji $t$ od $0$ do `max_iterations`-1:\n",
    "    a.  **Obliczenie gradientu:** Obliczany jest gradient $\\nabla F(\\mathbf{X}^{(t)})$. Jak wspomniano, gradienty dla $\\mathbf{x}(0)$ i $\\mathbf{x}(n)$ są ustawiane na $\\mathbf{0}$.\n",
    "    b.  **Kierunek spadku:** Kierunek poszukiwań $\\mathbf{d}^{(t)}$ jest wyznaczany jako ujemny gradient: $\\mathbf{d}^{(t)} = -\\nabla F(\\mathbf{X}^{(t)})$.\n",
    "    c.  **Przeszukiwanie liniowe (Metoda Złotego Podziału):** Wyznaczana jest optymalna długość kroku $\\alpha^{(t)}$, która minimalizuje funkcję jednowymiarową $\\phi(\\alpha) = F(\\mathbf{X}^{(t)} + \\alpha \\mathbf{d}^{(t)})$.\n",
    "        * **Metoda złotego podziału** iteracyjnie zawęża przedział, w którym znajduje się minimum funkcji unimodalnej, wykorzystując stałą proporcję złotego podziału do wyboru punktów testowych. Przykładowo, początkowy przedział dla $\\alpha$ może być określony jako $[0, 100]$. Iteracje kontynuuje się do momentu, gdy długość przedziału poszukiwań spadnie poniżej ustalonej tolerancji.\n",
    "    d.  **Aktualizacja ścieżki:** Nowa ścieżka $\\mathbf{X}^{(t+1)}$ jest obliczana jako: $\\mathbf{X}^{(t+1)} = \\mathbf{X}^{(t)} + \\alpha^{(t)} \\mathbf{d}^{(t)}$.\n",
    "    e.  **Utrwalenie punktów końcowych:** Po każdej aktualizacji, punkty $\\mathbf{x}(0)$ i $\\mathbf{x}(n)$ są ponownie ustawiane na ich początkowe, ustalone wartości, aby zapobiec ich zmianom.\n",
    "\n",
    "## 5. Analiza Kodu Implementacji Metody Największego Spadku\n",
    "\n",
    "Poniżej przedstawiono zwięzłą analizę kodu Python, który implementuje algorytm największego spadku z przeszukiwaniem liniowym (metodą złotego podziału) do optymalizacji ścieżki robota. Opis koncentruje się na funkcjonalności poszczególnych bloków, z wklejeniem omawianego fragmentu kodu i **bez komentarzy w samych blokach kodu**.\n",
    "\n",
    "### 5.1. Konfiguracja Środowiska i Parametry Problemowe\n",
    "\n",
    "Ta sekcja odpowiada za przygotowanie środowiska obliczeniowego, importy bibliotek i definicję stałych używanych w całym programie.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_segments = 20\n",
    "n_points = n_segments + 1\n",
    "k_obstacles = 50\n",
    "x0_fixed = np.array([0.0, 0.0])\n",
    "xn_fixed = np.array([20.0, 20.0])\n",
    "lambda1 = 1.0\n",
    "lambda2 = 1.0\n",
    "epsilon = 1e-13\n",
    "max_iterations = 400\n",
    "\n",
    "obstacles = np.random.uniform(0, 20, size=(k_obstacles, 2))\n",
    "```\n",
    "\n",
    "* **Importy Bibliotek:** Wczytuje podstawowe moduły: **`numpy`** do obliczeń numerycznych, **`matplotlib.pyplot`** do tworzenia wykresów i **`time`** do mierzenia czasu.\n",
    "* **Ustawienie Ziarna Losowości:** `np.random.seed(42)` zapewnia **powtarzalność wyników** eksperymentów, gwarantując, że losowe dane (np. przeszkody) będą takie same przy każdym uruchomieniu kodu.\n",
    "* **Definicja Parametrów:** Ustalane są kluczowe wartości problemu, takie jak: liczba odcinków ścieżki (`n_segments`), liczba przeszkód (`k_obstacles`), stałe punkty startowy (`x0_fixed`) i końcowy (`xn_fixed`) ścieżki, wagi (`lambda1`, `lambda2`) dla obu członów funkcji kosztu, mała stała `epsilon` (zapobiegająca dzieleniu przez zero) oraz maksymalna liczba iteracji algorytmu (`max_iterations`).\n",
    "* **Generowanie Przeszkód:** Tablica losowych przeszkód (`obstacles`) jest tworzona w zadanym zakresie współrzędnych, symulując środowisko robota.\n",
    "\n",
    "### 5.2. Obliczanie Wartości Funkcji Celu $F(X)$\n",
    "\n",
    "Funkcja `objective_function` jest centralnym elementem optymalizacji, ponieważ kwantyfikuje \"koszt\" danej ścieżki robota.\n",
    "\n",
    "```python\n",
    "def objective_function(X, obstacles, lambda1, lambda2, epsilon, n_segments, k_obstacles):\n",
    "    X_reshaped = X.reshape(n_segments + 1, 2)\n",
    "\n",
    "    term1_sum = 0\n",
    "    for i in range(n_segments + 1):\n",
    "        for j in range(k_obstacles):\n",
    "            dist_sq = np.sum((X_reshaped[i] - obstacles[j])**2)\n",
    "            term1_sum += 1 / (epsilon + dist_sq)\n",
    "    F1 = lambda1 * term1_sum\n",
    "\n",
    "    term2_sum = 0\n",
    "    for i in range(n_segments):\n",
    "        term2_sum += np.sum((X_reshaped[i+1] - X_reshaped[i])**2)\n",
    "    F2 = lambda2 * term2_sum\n",
    "\n",
    "    return F1 + F2\n",
    "```\n",
    "\n",
    "* **Cel:** Oblicza całkowitą wartość funkcji celu $F(\\mathbf{X})$ dla zadanej ścieżki robota $\\mathbf{X}$.\n",
    "* **Działanie:**\n",
    "    * Przyjmuje ścieżkę (`X`) w spłaszczonej formie i przekształca ją do macierzy punktów `(n+1)x2`.\n",
    "    * **Człon 1 (Unikanie Przeszkód):** Iteruje przez każdy punkt ścieżki i każdą przeszkodę. Oblicza kwadrat odległości, a następnie sumuje odwrotności tych odległości (skalowane przez `lambda1`). Bliższe odległości generują wyższy koszt.\n",
    "    * **Człon 2 (Długość Ścieżki):** Sumuje kwadraty długości każdego odcinka ścieżki (skalowane przez `lambda2`). Składnik ten \"nagradza\" krótsze ścieżki.\n",
    "    * Zwraca sumę kosztów z obu członów, reprezentującą ogólną \"nieatrakcyjność\" danej ścieżki.\n",
    "\n",
    "### 5.3. Obliczanie Gradientu $\\nabla F$\n",
    "\n",
    "Funkcja `gradient_F` jest fundamentalna dla algorytmu największego spadku, ponieważ wskazuje kierunek, w którym funkcja celu maleje najszybciej.\n",
    "\n",
    "```python\n",
    "def gradient_F(X, obstacles, lambda1, lambda2, epsilon, n_segments, k_obstacles):\n",
    "    X_reshaped = X.reshape(n_segments + 1, 2)\n",
    "    grad = np.zeros_like(X_reshaped)\n",
    "\n",
    "    for p in range(n_segments + 1):\n",
    "        grad_F1_p = np.zeros(2)\n",
    "        for j in range(k_obstacles):\n",
    "            diff = X_reshaped[p] - obstacles[j]\n",
    "            dist_sq = np.sum(diff**2)\n",
    "            grad_F1_p += -2 * diff / ((epsilon + dist_sq)**2)\n",
    "        grad_F1_p *= lambda1\n",
    "\n",
    "        grad_F2_p = np.zeros(2)\n",
    "        if p == 0:\n",
    "            grad_F2_p = -2 * (X_reshaped[1] - X_reshaped[0])\n",
    "        elif p == n_segments:\n",
    "            grad_F2_p = 2 * (X_reshaped[n_segments] - X_reshaped[n_segments-1])\n",
    "        else:\n",
    "            grad_F2_p = 2 * (X_reshaped[p] - X_reshaped[p-1]) - 2 * (X_reshaped[p+1] - X_reshaped[p])\n",
    "        grad_F2_p *= lambda2\n",
    "        \n",
    "        grad[p] = grad_F1_p + grad_F2_p\n",
    "\n",
    "    grad[0, :] = 0.0\n",
    "    grad[n_segments, :] = 0.0\n",
    "\n",
    "    return grad.flatten()\n",
    "```\n",
    "\n",
    "* **Cel:** Obliczenie wektora gradientu $\\nabla F$ dla wszystkich punktów ścieżki $\\mathbf{X}$.\n",
    "* **Działanie:**\n",
    "    * Inicjalizuje macierz gradientu, która będzie przechowywać pochodne cząstkowe dla każdego punktu ścieżki.\n",
    "    * Dla każdego punktu $\\mathbf{x}(p)$ na ścieżce, oblicza jego wkład do gradientu, sumując pochodne cząstkowe z członu unikania przeszkód i członu długości ścieżki, zgodnie z wyprowadzonymi wcześniej wzorami.\n",
    "    * **Kluczowe jest wyzerowanie gradientów dla punktów początkowego (`x(0)`) i końcowego (`x(n)`)**. Dzięki temu te punkty pozostaną stałe i nie będą zmieniane w trakcie optymalizacji.\n",
    "    * Zwraca obliczony gradient jako spłaszczony wektor.\n",
    "\n",
    "### 5.4. Przeszukiwanie Liniowe (Metoda Złotego Podziału)\n",
    "\n",
    "Funkcja `golden_section_search` jest algorytmem do znajdowania optymalnej długości kroku w kierunku gradientu.\n",
    "\n",
    "```python\n",
    "def golden_section_search(func, a, b, tol=1e-6):\n",
    "    phi = (1 + np.sqrt(5)) / 2\n",
    "    resphi = 2 - phi\n",
    "\n",
    "    x1 = a + resphi * (b - a)\n",
    "    x2 = b - resphi * (b - a)\n",
    "\n",
    "    f1 = func(x1)\n",
    "    f2 = func(x2)\n",
    "\n",
    "    while abs(b - a) > tol:\n",
    "        if f1 < f2:\n",
    "            b = x2\n",
    "            x2 = x1\n",
    "            f2 = f1\n",
    "            x1 = a + resphi * (b - a)\n",
    "            f1 = func(x1)\n",
    "        else:\n",
    "            a = x1\n",
    "            x1 = x2\n",
    "            f1 = f2\n",
    "            x2 = b - resphi * (b - a)\n",
    "            f2 = func(x2)\n",
    "    return (a + b) / 2\n",
    "```\n",
    "\n",
    "* **Cel:** Znalezienie optymalnej długości kroku `alpha` w danym kierunku, która minimalizuje funkcję `func` (jednowymiarową) w zadanym przedziale `[a, b]`.\n",
    "* **Działanie:** Algorytm złotego podziału iteracyjnie zawęża przedział, w którym znajduje się minimum funkcji. Wykorzystuje właściwości złotej proporcji do efektywnego wyboru punktów testowych. Proces kontynuuje się, aż przedział poszukiwań stanie się wystarczająco mały (poniżej tolerancji `tol`), a następnie zwraca środek tego przedziału jako optymalną wartość `alpha`.\n",
    "\n",
    "### 5.5. Główny Algorytm Największego Spadku\n",
    "\n",
    "Funkcja `gradient_descent_with_line_search` koordynuje cały proces optymalizacji ścieżki robota.\n",
    "\n",
    "```python\n",
    "def gradient_descent_with_line_search(initial_path_flat, obstacles, lambda1, lambda2, epsilon, n_segments, k_obstacles, max_iterations):\n",
    "    path_X_flat = np.copy(initial_path_flat)\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        current_F_value = objective_function(path_X_flat, obstacles, lambda1, lambda2, epsilon, n_segments, k_obstacles)\n",
    "        losses.append(current_F_value)\n",
    "\n",
    "        grad = gradient_F(path_X_flat, obstacles, lambda1, lambda2, epsilon, n_segments, k_obstacles)\n",
    "        \n",
    "        direction = -grad\n",
    "        \n",
    "        line_search_func = lambda alpha: objective_function(path_X_flat + alpha * direction, obstacles, lambda1, lambda2, epsilon, n_segments, k_obstacles)\n",
    "\n",
    "        alpha_optimal = golden_section_search(line_search_func, 0, 100)\n",
    "\n",
    "        path_X_flat += alpha_optimal * direction\n",
    "\n",
    "        path_X_flat[0:2] = x0_fixed\n",
    "        path_X_flat[-2:] = xn_fixed\n",
    "        \n",
    "    return path_X_flat.reshape(n_points, 2), losses\n",
    "```\n",
    "\n",
    "* **Cel:** Iteracyjna minimalizacja funkcji celu $F(\\mathbf{X})$, aby znaleźć optymalną ścieżkę robota.\n",
    "* **Działanie:**\n",
    "    * Rozpoczyna od kopii początkowej ścieżki i pustej listy na wartości funkcji celu.\n",
    "    * W głównej pętli (do `max_iterations`):\n",
    "        * Oblicza bieżącą wartość funkcji celu i dodaje ją do listy `losses`.\n",
    "        * Oblicza **gradient** dla obecnej ścieżki.\n",
    "        * Określa **kierunek spadku** (przeciwny do gradientu).\n",
    "        * Definiuje jednowymiarową funkcję, która pozwoli `golden_section_search` znaleźć najlepszą długość kroku (`alpha_optimal`) w tym kierunku.\n",
    "        * **Aktualizuje ścieżkę**, przesuwając wszystkie punkty o `alpha_optimal` w kierunku spadku.\n",
    "        * **Resetuje współrzędne punktów początkowego (`x(0)`) i końcowego (`x(n)`)** do ich stałych wartości, upewniając się, że nie ulegają one zmianie.\n",
    "    * Po zakończeniu iteracji zwraca zoptymalizowaną ścieżkę i historię wartości funkcji celu.\n",
    "\n",
    "### 5.6. Uruchomienie i Wizualizacja Wyników\n",
    "\n",
    "Ta sekcja odpowiada za wykonanie symulacji optymalizacyjnych i wizualizację uzyskanych wyników.\n",
    "\n",
    "```python\n",
    "num_runs = 5\n",
    "results = []\n",
    "fig, axes = plt.subplots(1, num_runs, figsize=(num_runs * 5, 6))\n",
    "fig.suptitle(\"Zoptymalizowane ścieżki robota dla różnych inicjalizacji\", fontsize=16)\n",
    "\n",
    "if num_runs == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for run_idx in range(num_runs):\n",
    "    print(f\"\\n--- Uruchomienie {run_idx + 1} ---\")\n",
    "    \n",
    "    initial_path_interior = np.random.uniform(0, 20, size=(n_points - 2, 2))\n",
    "    \n",
    "    initial_path_X = np.zeros((n_points, 2))\n",
    "    initial_path_X[0] = x0_fixed\n",
    "    initial_path_X[n_points-1] = xn_fixed\n",
    "    initial_path_X[1:n_points-1] = initial_path_interior\n",
    "    \n",
    "    start_time = time.time()\n",
    "    optimized_path, losses = gradient_descent_with_line_search(\n",
    "        initial_path_X.flatten(), obstacles, lambda1, lambda2, epsilon, n_segments, k_obstacles, max_iterations\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    results.append({\n",
    "        \"optimized_path\": optimized_path,\n",
    "        \"losses\": losses,\n",
    "        \"time\": end_time - start_time,\n",
    "        \"initial_path\": initial_path_X\n",
    "    })\n",
    "\n",
    "    print(f\"Czas obliczeń: {end_time - start_time:.4f} s\")\n",
    "    print(f\"Wartość funkcji F po optymalizacji: {losses[-1]:.4f}\")\n",
    "\n",
    "    ax = axes[run_idx]\n",
    "    ax.plot(optimized_path[:, 0], optimized_path[:, 1], 'b-o', markersize=3, label=\"Zoptymalizowana ścieżka\")\n",
    "    ax.plot(obstacles[:, 0], obstacles[:, 1], 'rx', markersize=8, label=\"Przeszkody\")\n",
    "    ax.plot(x0_fixed[0], x0_fixed[1], 'go', markersize=8, label=\"Start x(0)\")\n",
    "    ax.plot(xn_fixed[0], xn_fixed[1], 'ko', markersize=8, label=\"Koniec x(n)\")\n",
    "    ax.set_title(f\"Run {run_idx + 1}\\nF_final={losses[-1]:.2f}\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.grid(True)\n",
    "    if run_idx == 0:\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results[0][\"losses\"], label=\"Wartość funkcji F\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteracja\")\n",
    "plt.ylabel(\"Wartość funkcji F (log)\")\n",
    "plt.title(\"Zbieżność funkcji celu F w zależności od iteracji (dla pierwszej inicjalizacji)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "* **Cel:** Wykonać wielokrotne serie optymalizacji i zwizualizować zoptymalizowane ścieżki oraz proces zbieżności funkcji kosztu.\n",
    "* **Działanie:**\n",
    "    * Algorytm jest uruchamiany określoną liczbę razy (`num_runs`), każdorazowo z inną **losową inicjalizacją** punktów wewnętrznych ścieżki, co pozwala ocenić stabilność i powtarzalność wyników.\n",
    "    * Dla każdego uruchomienia:\n",
    "        * Przygotowuje się nową ścieżkę początkową z losowymi punktami pośrednimi.\n",
    "        * Mierzony jest **czas** trwania optymalizacji.\n",
    "        * Wywoływana jest funkcja `gradient_descent_with_line_search` w celu zoptymalizowania ścieżki.\n",
    "        * Wyniki (zoptymalizowana ścieżka, wartości funkcji kosztu, czas) są zapisywane.\n",
    "        * Generowany jest wykres pokazujący **zoptymalizowaną ścieżkę**, przeszkody oraz punkty startowy i końcowy.\n",
    "    * Po wszystkich uruchomieniach, tworzony jest oddzielny wykres przedstawiający **zbieżność wartości funkcji celu** w kolejnych iteracjach (zazwyczaj w skali logarytmicznej), co wizualizuje, jak funkcja kosztu maleje podczas optymalizacji.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Wyniki Optymalizacji\n",
    "\n",
    "Algorytm został zastosowany z następującymi parametrami:\n",
    "* Liczba odcinków ścieżki $n=20$ (co daje $21$ punktów).\n",
    "* Liczba przeszkód $k=50$.\n",
    "* Punkt początkowy $\\mathbf{x}(0) = [0,0]$.\n",
    "* Punkt końcowy $\\mathbf{x}(n) = [20,20]$.\n",
    "* Położenia przeszkód $\\mathbf{r}(j)$ zostały wylosowane z rozkładu jednostajnego $U(0,20) \\times U(0,20)$.\n",
    "* Wagi członów funkcji celu: $\\lambda_1 = 1$, $\\lambda_2 = 1$.\n",
    "* Stała zabezpieczająca: $\\epsilon = 10^{-13}$.\n",
    "* Maksymalna liczba iteracji: $400$.\n",
    "* Dla zapewnienia powtarzalności wyników, ziarno generatora liczb losowych zostało ustawione.\n",
    "\n",
    "Obliczenia zostały przeprowadzone dla **pięciu różnych losowych inicjalizacji** punktów wewnętrznych ścieżki ($\\mathbf{x}(1), \\dots, \\mathbf{x}(n-1)$).\n",
    "\n",
    "#### 6.1. Czas szukania ścieżek\n",
    "\n",
    "Poniżej przedstawiono końcowy czas wykonmywanych obliczeń dla każdej iteracji:\n",
    "\n",
    "```\n",
    "--- Uruchomienie 1 ---\n",
    "Czas obliczeń: 70.6686 s\n",
    "Wartość funkcji F po optymalizacji: 76.9744\n",
    "\n",
    "--- Uruchomienie 2 ---\n",
    "Czas obliczeń: 69.9598 s\n",
    "Wartość funkcji F po optymalizacji: 74.7293\n",
    "\n",
    "--- Uruchomienie 3 ---\n",
    "Czas obliczeń: 69.7139 s\n",
    "Wartość funkcji F po optymalizacji: 73.1258\n",
    "\n",
    "--- Uruchomienie 4 ---\n",
    "Czas obliczeń: 69.9671 s\n",
    "Wartość funkcji F po optymalizacji: 76.7375\n",
    "\n",
    "--- Uruchomienie 5 ---\n",
    "Czas obliczeń: 71.5286 s\n",
    "Wartość funkcji F po optymalizacji: 73.7737\n",
    "```\n",
    "\n",
    "#### 6.2. Wizualizacja Zoptymalizowanych Ścieżek\n",
    "\n",
    "Poniżej przedstawiono przykładowe wizualizacje zoptymalizowanych ścieżek dla różnych inicjalizacji. Każdy wykres ilustruje punkty startowe, docelowe, rozmieszczenie przeszkód oraz wyznaczoną ścieżkę robota.\n",
    "\n",
    "![image](./assets/results.png)\n",
    "\n",
    "Analiza wizualna wykresów pokazuje, że algorytm skutecznie prowadzi robota od punktu startowego do końcowego, jednocześnie omijając zdefiniowane przeszkody. Ścieżki są płynne i efektywne, co świadczy o prawidłowym działaniu obu członów funkcji celu.\n",
    "\n",
    "#### 6.3. Zbieżność Funkcji Celu\n",
    "\n",
    "Wartość funkcji celu $F(\\mathbf{X})$ jest monitorowana w każdej iteracji. Poniższy wykres przedstawia typową zbieżność wartości funkcji $F$ w zależności od liczby iteracji dla jednej z inicjalizacji.\n",
    "\n",
    "![image](./assets/result_2.png)\n",
    "\n",
    "\n",
    "Wykres zbieżności (często prezentowany w skali logarytmicznej dla wartości funkcji celu) demonstruje, że algorytm największego spadku efektywnie minimalizuje funkcję kosztu. Wartość $F$ szybko spada w początkowych iteracjach, a następnie stabilizuje się, co wskazuje na osiągnięcie lokalnego minimum.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
